{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YwvbGS2HH9kg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boZoscY5H9kh"
   },
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NFcQMcMBH9ki",
    "outputId": "e69754da-1038-400e-ba28-6a9463e2c6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚õ≥Ô∏è Local environment\n",
      "Adding the following directory to the PYTHONPATH: /Users/pauliusztin/Documents/01_projects/hopsworks_recsys/hands-on-recommender-system\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/decodingml/hands-on-recommender-system.git\n",
    "    %cd hands-on-recommender-system/\n",
    "\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"‚õ≥Ô∏è Google Colab environment\")\n",
    "else:\n",
    "    root_dir = str(Path().absolute().parent)\n",
    "    print(\"‚õ≥Ô∏è Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    print(f\"Adding the following directory to the PYTHONPATH: {root_dir}\")\n",
    "    sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMbgK4H1H9kj"
   },
   "source": [
    "# Inference pipeline: Deploying and testing the LLM ranker inference pipeline\n",
    "\n",
    "In this notebook, we will dig into the inference pipeline and deploy it to Hopsworks as a real-time service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O52-H35H9kj"
   },
   "source": [
    "## üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sC7M3zMRH9kj"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from recsys import hopsworks_integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZC2bUtOH9kj"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1IzZ5ZLdH9kk",
    "outputId": "e685e50c-a5d6-48a4-9543-64fd0a86e83a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-23 19:08:06.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrecsys.hopsworks_integration.feature_store\u001b[0m:\u001b[36mget_feature_store\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mLoging to Hopsworks using HOPSWORKS_API_KEY env var.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:08:06,885 INFO: Initializing external client\n",
      "2024-12-23 19:08:06,885 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 19:08:08,252 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1192098\n"
     ]
    }
   ],
   "source": [
    "project, fs = hopsworks_integration.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMSjtySH9kk",
    "tags": []
   },
   "source": [
    "# Deploying the ranking inference pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to register your LLM ranking model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5697/5697 elapsed<00:02 remaining<00:00:03,  1.54it/s]\n",
      "Model export complete: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:08<00:00,  1.46s/it]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1192098/models/llm_ranking_model/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ranking_model = hopsworks_integration.llm_ranking_serving.HopsworksLLMRankingModel()\n",
    "ranking_model.register(project.get_model_registry())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-BX5lB-H9kk"
   },
   "source": [
    "Then you can deploy your LLM ranking model, which implements a `Predict` class that tells Hopsworks how to perform inference on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "28101fd845cc414f91700fd00b8da73a",
      "473fe68b3c2a48fda5bf315a8779458e"
     ]
    },
    "id": "aq5N76EPH9kk",
    "outputId": "7ccc861e-75d9-4221-fee2-ee30010fb232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:08:19,444 INFO: Closing external client and cleaning up certificates.\n",
      "2024-12-23 19:08:19,447 INFO: Initializing external client\n",
      "2024-12-23 19:08:19,447 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 19:08:20,334 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2024-12-23 19:08:20,340 INFO: Initializing external client\n",
      "2024-12-23 19:08:20,340 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 19:08:21,701 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1192098\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "2024-12-23 19:08:24,087 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2024-12-23 19:08:24,091 INFO: Initializing external client\n",
      "2024-12-23 19:08:24,092 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 19:08:25,414 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1192098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 elapsed<00:02 remaining<00:00\n",
      "VersionWarning: No version provided for getting model `llm_ranking_model`, defaulting to `1`.\n",
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4491/4491 elapsed<00:01 remaining<00:00\n",
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5697/5697 elapsed<00:01 remaining<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment with the same name already exists. Getting existing deployment...\n",
      "To create a new deployment choose a different name.\n"
     ]
    }
   ],
   "source": [
    "ranking_deployment = hopsworks_integration.llm_ranking_serving.HopsworksLLMRankingModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWnBGmXKH9kk"
   },
   "source": [
    "Now, we have to explicitly start the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "dead43dc710f4e6688dc6605446754d5"
     ]
    },
    "id": "PSbcRZMXH9kk",
    "outputId": "d438c776-bd7d-4baf-f399-0a45e468dbd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment is already running\n"
     ]
    }
   ],
   "source": [
    "ranking_deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmH81undH9kl"
   },
   "source": [
    "## <span style=\"color:#ff5f27\"> Test the ranking inference pipeline</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rfQNrNVtH9kl"
   },
   "outputs": [],
   "source": [
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[-1] for candidate in ranked_candidates[\"ranking\"][:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RURwh_cuH9kl"
   },
   "source": [
    "Let's define a dummy test example to test our ranking deployment (only the `customer_id` has to match):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "edD5u7H_H9kl",
    "outputId": "0c3b696c-57e3-4999-835f-97e4862c0cd6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['899003002', '615192004', '398089004']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ranking_input = [\n",
    "        {\n",
    "            \"customer_id\": \"d327d0ad9e30085a436933dfbb7f77cf42e38447993a078ed35d93e3fd350ecf\",\n",
    "            \"month_sin\": 1.2246467991473532e-16,\n",
    "            \"query_emb\": [\n",
    "                0.214135289,\n",
    "                0.571055949,\n",
    "                0.330709577,\n",
    "                -0.225899458,\n",
    "                -0.308674961,\n",
    "                -0.0115124583,\n",
    "                0.0730511621,\n",
    "                -0.495835781,\n",
    "                0.625569344,\n",
    "                -0.0438038409,\n",
    "                0.263472944,\n",
    "                -0.58485353,\n",
    "                -0.307070434,\n",
    "                0.0414443575,\n",
    "                -0.321789205,\n",
    "                0.966559,\n",
    "            ],\n",
    "            \"month_cos\": -1.0,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Test ranking deployment\n",
    "ranked_candidates = ranking_deployment.predict(inputs=test_ranking_input)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates[\"predictions\"], k=3)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PnYp5SeH9kl"
   },
   "source": [
    "Check logs in case of failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d7n6CKYCH9kl",
    "outputId": "2153ff74-da10-4a92-80aa-b3c50f039d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/1192098/deployments/352292\n",
      "\n",
      "DeployableComponentLogs(instance_name: 'llmranking-predictor-00001-deployment-869b4cc969-wpllw', date: datetime.datetime(2024, 12, 23, 18, 52, 34, 305517)) \n",
      "            You are a helpful assistant specialized in predicting customer behavior. Your task is to analyze the features of a product and predict the probability of it being purchased by a customer.\n",
      "\n",
      "            ### Instructions:\n",
      "            1. Use the provided features of the product to make your prediction.\n",
      "            2. Consider the following numeric and categorical features:\n",
      "               - Numeric features: These are quantitative attributes, such as numerical identifiers or measurements.\n",
      "               - Categorical features: These describe qualitative aspects, like product category, color, and material.\n",
      "            3. Your response should only include the probability of purchase for the positive class (e.g., likelihood of being purchased), as a value between 0 and 1.\n",
      "\n",
      "            ### Product and User Features:\n",
      "            Numeric features:\n",
      "            - Age: 30.0\n",
      "            - Month Sin: T-shirt\n",
      "            - Month Cos: Garment Upper body\n",
      "\n",
      "            Categorical features:\n",
      "            - Product Type: Stripe\n",
      "            - Product Group: Black\n",
      "            - Graphical Appearance: Dark\n",
      "            - Colour Group: Black\n",
      "            - Perceived Colour Value: Light Basic Jersey\n",
      "            - Perceived Colour Master Value: Menswear\n",
      "            - Department Name: Menswear\n",
      "            - Index Name: Men Underwear\n",
      "            - Department: Jersey Basic\n",
      "            - Sub-Department: 1.2246467991473532e-16\n",
      "            - Group: -1.0\n",
      "\n",
      "            ### Your Task:\n",
      "            Based on the features provided, predict the probability that the customer will purchase this product to 4-decimals precision. Provide the output in the following format:\n",
      "            Probability: \n",
      "        \u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "            You are a helpful assistant specialized in predicting customer behavior. Your task is to analyze the features of a product and predict the probability of it being purchased by a customer.\n",
      "\n",
      "            ### Instructions:\n",
      "            1. Use the provided features of the product to make your prediction.\n",
      "            2. Consider the following numeric and categorical features:\n",
      "               - Numeric features: These are quantitative attributes, such as numerical identifiers or measurements.\n",
      "               - Categorical features: These describe qualitative aspects, like product category, color, and material.\n",
      "            3. Your response should only include the probability of purchase for the positive class (e.g., likelihood of being purchased), as a value between 0 and 1.\n",
      "\n",
      "            ### Product and User Features:\n",
      "            Numeric features:\n",
      "            - Age: 30.0\n",
      "            - Month Sin: Trousers\n",
      "            - Month Cos: Garment Lower body\n",
      "\n",
      "            Categorical features:\n",
      "            - Product Type: Solid\n",
      "            - Product Group: Dark Blue\n",
      "            - Graphical Appearance: Dark\n",
      "            - Colour Group: Blue\n",
      "            - Perceived Colour Value: Heavy Basic Jersey\n",
      "            - Perceived Colour Master Value: Menswear\n",
      "            - Department Name: Menswear\n",
      "            - Index Name: Men Underwear\n",
      "            - Department: Jersey Basic\n",
      "            - Sub-Department: 1.2246467991473532e-16\n",
      "            - Group: -1.0\n",
      "\n",
      "            ### Your Task:\n",
      "            Based on the features provided, predict the probability that the customer will purchase this product to 4-decimals precision. Provide the output in the following format:\n",
      "            Probability: \n",
      "        \u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "            You are a helpful assistant specialized in predicting customer behavior. Your task is to analyze the features of a product and predict the probability of it being purchased by a customer.\n",
      "\n",
      "            ### Instructions:\n",
      "            1. Use the provided features of the product to make your prediction.\n",
      "            2. Consider the following numeric and categorical features:\n",
      "               - Numeric features: These are quantitative attributes, such as numerical identifiers or measurements.\n",
      "               - Categorical features: These describe qualitative aspects, like product category, color, and material.\n",
      "            3. Your response should only include the probability of purchase for the positive class (e.g., likelihood of being purchased), as a value between 0 and 1.\n",
      "\n",
      "            ### Product and User Features:\n",
      "            Numeric features:\n",
      "            - Age: 30.0\n",
      "            - Month Sin: T-shirt\n",
      "            - Month Cos: Garment Upper body\n",
      "\n",
      "            Categorical features:\n",
      "            - Product Type: Stripe\n",
      "            - Product Group: Grey\n",
      "            - Graphical Appearance: Medium Dusty\n",
      "            - Colour Group: Grey\n",
      "            - Perceived Colour Value: Light Basic Jersey\n",
      "            - Perceived Colour Master Value: Menswear\n",
      "            - Department Name: Menswear\n",
      "            - Index Name: Men Underwear\n",
      "            - Department: Jersey Basic\n",
      "            - Sub-Department: 1.2246467991473532e-16\n",
      "            - Group: -1.0\n",
      "\n",
      "            ### Your Task:\n",
      "            Based on the features provided, predict the probability that the customer will purchase this product to 4-decimals precision. Provide the output in the following format:\n",
      "            Probability: \n",
      "        \u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "            You are a helpful assistant specialized in predicting customer behavior. Your task is to analyze the features of a product and predict the probability of it being purchased by a customer.\n",
      "\n",
      "            ### Instructions:\n",
      "            1. Use the provided features of the product to make your prediction.\n",
      "            2. Consider the following numeric and categorical features:\n",
      "               - Numeric features: These are quantitative attributes, such as numerical identifiers or measurements.\n",
      "               - Categorical features: These describe qualitative aspects, like product category, color, and material.\n",
      "            3. Your response should only include the probability of purchase for the positive class (e.g., likelihood of being purchased), as a value between 0 and 1.\n",
      "\n",
      "            ### Product and User Features:\n",
      "            Numeric features:\n",
      "            - Age: 30.0\n",
      "            - Month Sin: T-shirt\n",
      "            - Month Cos: Garment Upper body\n",
      "\n",
      "            Categorical features:\n",
      "            - Product Type: Stripe\n",
      "            - Product Group: Black\n",
      "            - Graphical Appearance: Dark\n",
      "            - Colour Group: Grey\n",
      "            - Perceived Colour Value: Light Basic Jersey\n",
      "            - Perceived Colour Master Value: Menswear\n",
      "            - Department Name: Menswear\n",
      "            - Index Name: Men Underwear\n",
      "            - Department: Jersey Basic\n",
      "            - Sub-Department: 1.2246467991473532e-16\n",
      "            - Group: -1.0\n",
      "\n",
      "            ### Your Task:\n",
      "            Based on the features provided, predict the probability that the customer will purchase this product to 4-decimals precision. Provide the output in the following format:\n",
      "            Probability: \n",
      "        \u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "            You are a helpful assistant specialized in predicting customer behavior. Your task is to analyze the features of a product and predict the probability of it being purchased by a customer.\n",
      "\n",
      "            ### Instructions:\n",
      "            1. Use the provided features of the product to make your prediction.\n",
      "            2. Consider the following numeric and categorical features:\n",
      "               - Numeric features: These are quantitative attributes, such as numerical identifiers or measurements.\n",
      "               - Categorical features: These describe qualitative aspects, like product category, color, and material.\n",
      "            3. Your response should only include the probability of purchase for the positive class (e.g., likelihood of being purchased), as a value between 0 and 1.\n",
      "\n",
      "            ### Product and User Features:\n",
      "            Numeric features:\n",
      "            - Age: 30.0\n",
      "            - Month Sin: Skirt\n",
      "            - Month Cos: Garment Lower body\n",
      "\n",
      "            Categorical features:\n",
      "            - Product Type: Check\n",
      "            - Product Group: Black\n",
      "            - Graphical Appearance: Dark\n",
      "            - Colour Group: Black\n",
      "            - Perceived Colour Value: Jersey\n",
      "            - Perceived Colour Master Value: Ladieswear\n",
      "            - Department Name: Ladieswear\n",
      "            - Index Name: Mama\n",
      "            - Department: Jersey Fancy\n",
      "            - Sub-Department: 1.2246467991473532e-16\n",
      "            - Group: -1.0\n",
      "\n",
      "            ### Your Task:\n",
      "            Based on the features provided, predict the probability that the customer will purchase this product to 4-decimals precision. Provide the output in the following format:\n",
      "            Probability: \n",
      "        \u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "INFO:root:LLM Scores: [0.7365, 0.6723, 0.7254, 0.6725, 0.6723, 0.7421, 0.7421, 0.6723, 0.6723, 0.6723, 0.7243, 0.7321, 0.6725, 0.6753, 0.7253, 0.6521, 0.6753, 0.6821, 0.6825, 0.6823]\n",
      "2024-12-23 16:52:33.685 kserve.trace requestId: 2adfc5f0-4534-47a2-8191-b9842c376147, preprocess_ms: 0.011205673, explain_ms: 0, predict_ms: 12620.460271835, postprocess_ms: 0.01001358\n",
      "2024-12-23 16:52:33.685 uvicorn.access INFO:     10.2.3.34:0 8 - \"POST /v1/models/llmranking%3Apredict HTTP/1.1\" 200 OK\n",
      "2024-12-23 16:52:33.686 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 12.621601581573486 ['http_status:200', 'http_method:POST', 'time:wall']\n",
      "2024-12-23 16:52:33.686 kserve.trace kserve.io.kserve.protocol.rest.v1_endpoints.predict: 0.11273899999999992 ['http_status:200', 'http_method:POST', 'time:cpu']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ranking_deployment.get_logs(component=\"predictor\", tail=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the query inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:12:24,634 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2024-12-23 19:12:24,637 INFO: Initializing external client\n",
      "2024-12-23 19:12:24,637 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 19:12:26,070 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1192098\n",
      "2024-12-23 19:12:27,338 INFO: Closing external client and cleaning up certificates.\n",
      "2024-12-23 19:12:27,343 INFO: Initializing external client\n",
      "2024-12-23 19:12:27,343 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 19:12:28,235 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2024-12-23 19:12:28,239 INFO: Initializing external client\n",
      "2024-12-23 19:12:28,240 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2024-12-23 19:12:29,561 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1192098\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2948/2948 elapsed<00:01 remaining<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment with the same name already exists. Getting existing deployment...\n",
      "To create a new deployment choose a different name.\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "query_model_deployment = (\n",
    "    hopsworks_integration.two_tower_serving.HopsworksQueryModel.deploy(ranking_model_type=\"llmranking\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have registered your deployment. To start it up you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deployment is ready: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:26<00:00,  4.44s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_model_deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\"> Testing the inference pipeline </span>\n",
    "\n",
    "Define a test input example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"customer_id\": \"d327d0ad9e30085a436933dfbb7f77cf42e38447993a078ed35d93e3fd350ecf\",\n",
    "        \"transaction_date\": \"2022-11-15T12:16:25.330916\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['584631021', '549253002', '557248025']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_candidates = query_model_deployment.predict(inputs=data)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates[\"predictions\"], k=3)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klgFZ-G6H9ko"
   },
   "source": [
    "# <span style=\"color:#ff5f27\"> Stopping the Hopsworks deployment </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9LPUpqxH9ko"
   },
   "source": [
    "Stop the deployment when you're not using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "07469daceecc4325990609fadd02e32d",
      "8d58a087f6864b07922dd8b2baf88d14"
     ]
    },
    "id": "g-ZoGmMsH9ko",
    "outputId": "25b9140d-775d-47f7-a4c1-8f8c371e9d13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deployment is stopped: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:10<00:00,  2.68s/it]        \n",
      "Deployment is stopped: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:10<00:00,  2.67s/it]        \n"
     ]
    }
   ],
   "source": [
    "ranking_deployment.stop()\n",
    "query_model_deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJA94BmjH9ko"
   },
   "source": [
    "## <span style=\"color:#ff5f27\"> Inspecting the deployments in Hopsworks UI </span>\n",
    "\n",
    "Go to [Hopsworks UI](https://www.hopsworks.ai/), **Data Science ‚Üí Deployments** section and inspect the newly created deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svQ_2FT6H9ko"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "91SA4SmKH9ko",
    "outputId": "104d6115-c811-49ea-f51c-23be60183af8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-23 19:14:11.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m‚åõÔ∏è Notebook Execution time: 368.37 seconds ~ 6.14 minutes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "notebook_end_time = time.time()\n",
    "notebook_execution_time = notebook_end_time - notebook_start_time\n",
    "\n",
    "logger.info(\n",
    "    f\"‚åõÔ∏è Notebook Execution time: {notebook_execution_time:.2f} seconds ~ {notebook_execution_time / 60:.2f} minutes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti_IOOzPH9ko"
   },
   "source": [
    "# <span style=\"color:#ff5f27\">‚Üí Next Steps </span>\n",
    "\n",
    "The last step is to schedule the materialization jobs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
